# Natural Language Processing (NLP) and generative AI course

Hands-on experience with foundational and advanced techniques in NLP and Generative AI. It covers:
- Traditional methods like Bag-of-Words and TF-IDF
- Word embeddings such as Word2Vec
- Advanced architectures like LSTM, Autoencoders, Seq2Seq, and BERT

## Techniques
- **Counting Tokens**: How to tokenize and count tokens in text data.
- **Bag-of-Words & TF-IDF**: Creating document-term matrices and extracting relevant features.
- **Word2Vec Embeddings**: Generating dense vector representations for words.
- **LSTM**: Using Long Short-Term Memory networks for sequence modeling.
- **Autoencoders**: Compressing and reconstructing text data using autoencoders.
- **Seq2Seq**: Implementing Sequence-to-Sequence models for tasks like translation.
- **BERT**: Fine-tuning Bidirectional Encoder Representations from Transformers for NLP tasks.

